<html>
  <body>
<table border="0">
  <tbody>
    <tr>
      <td width="75%">
        <p><b>SNN推荐文献：</b></p>
        <p><b> 李国齐组：<br/>
			<a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00331/full" target="_blank">STBP</a><br/>
			<a href="https://arxiv.org/abs/1809.05793" target="_blank">NeuNorm</a><br/>
			<a href="https://arxiv.org/abs/2011.05280" target="_blank">tdBN</a><br/>
			<a href="https://arxiv.org/abs/2011.06176" target="_blank">LIAF</a><br/>
			<a href="https://www.sciencedirect.com/science/article/pii/S0893608019302667" target="_blank">Rethinking the performance comparison between SNNS and ANNS</a><br/>
			<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yao_Temporal-Wise_Attention_Spiking_Neural_Networks_for_Event_Streams_Classification_ICCV_2021_paper.pdf" target="_blank">Temporal-wise Attention Spiking Neural Networks for Event Streams Classification</a><br/>
			<a href="https://arxiv.org/abs/2203.01158" target="_blank">Rethinking Pretraining as a Bridge from ANNs to SNNs</a>
		</b></p>
		<p><b> 唐华锦组：
			
		</b></p>
		<p><b> 其他：
			<a href="https://www.sciencedirect.com/science/article/pii/S0278612520301138" target="_blank">A spiking neural network-based approach to bearing fault diagnosis</a>
		</b></p>
      </td>
    </tr>
  </tbody>
</table>
<table border="0">
  <tbody>
    <tr>
      <td width="75%">
        <p><b>推荐阅读：</b></p>
        <p><b><a href="/resources/papers/Attention_is_all_you_need.pdf" target="_blank">Transformer</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			  <a href="/resources/papers/Swin Transformer.pdf" target="_blank">Swin Transformer</a></b></p>
        <p><b><a href="/resources/papers/Deep_Residual_Learning_for_Image_Recognition.pdf" target="_blank">Residual Network</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			  <a href="/resources/papers/Deep_Residual_Shrinkage_Networks_for_Fault_Diagnosis.pdf" target="_blank">Deep Residual Shrinkage Networks</a></b></p>
        <p><b><a href="/resources/papers/NIPS-2014-generative-adversarial-nets-Paper.pdf" target="_blank">GAN</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			  <a href="/resources/papers/Conditional Generative Adversarial Nets.pdf" target="_blank">CGAN</a></b></p>
        <p><b><a href="/resources/papers/How_Does_Batch_Normalization_Help_Optimization.pdf" target="_blank">Batch Normalization</a></b></p>
        <p><b><a href="/resources/papers/Identity_Mappings_in_Deep_Residual_Networks.pdf" target="_blank">Identity Mappings in Deep Residual Networks</a></b></p>
        <p><b><a href="/resources/papers/Masked Autoencoders Are Scalable Vision Learners.pdf" target="_blank">Masked Autoencoders Are Scalable Vision Learners</a></b></p>
        <p><b><a href="/resources/papers/Large_Minibatch_SGD.pdf" target="_blank">Large Minibatch SGD</a></b></p>
        <p><b><a href="/resources/papers/AlexNet.pdf" target="_blank">AlexNet</a></b></p>
        <p><b><a href="/resources/papers/BERT.pdf" target="_blank">BERT</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			  <a href="/resources/papers/ViT.pdf" target="_blank">ViT</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			  <a href="/resources/papers/Momentum Contrast for Unsupervised Visual Representation Learning.pdf" target="_blank">MoCo</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			  <a href="/resources/papers/icml-2008-denoising-autoencoders.pdf" target="_blank">DAE</a></b></p>
        <p><b><a href="/resources/papers/You Only Look Once.pdf" target="_blank">YOLOv1</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			  <a href="/resources/papers/YOLO9000.pdf" target="_blank">YOLOv2</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			  <a href="/resources/papers/YOLOv3.pdf" target="_blank">YOLOv3</a></b></p>
      </td>
    </tr>
  </tbody>
</table>

  </body>
</html>

